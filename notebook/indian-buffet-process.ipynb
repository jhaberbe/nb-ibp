{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8aec32",
   "metadata": {},
   "source": [
    "# Broad Structure\n",
    "\n",
    "Our sampler is going to have three components to it:\n",
    "\n",
    "1. Learning the sparse latent matrix\n",
    "    - We will encode this as draws from a bernoulli probability (logits).\n",
    "    - We should be able to use a polya gamma construction for this!\n",
    "\n",
    "2. Learning the loading matrix\n",
    "    - We will use our polya gamma construction here as well.\n",
    "\n",
    "3. Dispersion parameter\n",
    "    - Probably, we will use simple MH type updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8957dac",
   "metadata": {},
   "source": [
    "# Indian Buffet Process Class\n",
    "\n",
    "I think it should be a container for our latent factors, which will have all the coefficients and stuff. However, the math stuff will definitely be done on this thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ad604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhaberbe/Projects/Personal/nb-ibp/.venv/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:172: ImplicitModificationWarning: Trying to modify attribute `._uns` of view, initializing view as actual.\n",
      "  adata.uns[\"hvg\"] = {\"flavor\": flavor}\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "adata = sc.read_h5ad(\"/home/jhaberbe/Data/choroid-plexus/new_annotations.h5ad\")\n",
    "adata = adata[adata.X.sum(axis=1) > 300]\n",
    "adata = adata[adata.obs[\"Cell.Subtype\"].eq(\"Macrophage\")][::5]\n",
    "sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=1000, subset=True, inplace=True)\n",
    "\n",
    "X = adata.X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2123479",
   "metadata": {},
   "source": [
    "Actual sampling time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37409b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "from tqdm import tqdm, trange\n",
    "from pypolyagamma import PyPolyaGamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a7ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pypolyagamma import PyPolyaGamma\n",
    "\n",
    "class PolyaGammaSampler:\n",
    "    def __init__(self):\n",
    "        self.pg = PyPolyaGamma(seed=np.random.randint(69420))\n",
    "\n",
    "    def sample(self, b, c):\n",
    "        \"\"\"\n",
    "        Sample PG(b, c), element-wise over arrays.\n",
    "        \"\"\"\n",
    "        b = np.asarray(b, dtype=np.double)\n",
    "        c = np.asarray(c, dtype=np.double)\n",
    "        shape = c.shape\n",
    "\n",
    "        b_flat = b.flatten()\n",
    "        c_flat = c.flatten()\n",
    "        out = np.zeros_like(c_flat)\n",
    "        self.pg.pgdrawv(b_flat, c_flat, out)\n",
    "\n",
    "        return out.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndianBuffetProcess:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, counts: np.array, alpha: float = 1.0):\n",
    "        # Count Matrix, np.array\n",
    "        self.counts = counts\n",
    "        self.size_factors = np.log(counts.sum(axis=1) / counts.sum(axis=1).mean())\n",
    "\n",
    "        # Innovation Parameter\n",
    "        self.alpha = alpha / np.log(self.counts.shape[0])\n",
    "\n",
    "        # Latent Factor Matrix.\n",
    "        self.factors = {}\n",
    "\n",
    "        # Our prior dispersion parameter, right now it will be 5.0, but this can change.\n",
    "        self.dispersion = np.ones(self.counts.shape[1]) * 5.0\n",
    "\n",
    "        # Loading updates stuff for polya gamma sampler.\n",
    "        self.polya_gamma_sampler = PolyaGammaSampler()\n",
    "        self.prior_precision = 0.01 * np.ones(self.counts.shape[1])  # Precision vector for A_k\n",
    "\n",
    "\n",
    "    # This section will deal with factors broadly.\n",
    "    def add_factor(self, index: Union[None, int]):\n",
    "        i = 0\n",
    "        while i in self.factors:\n",
    "            i += 1\n",
    "\n",
    "        self.factors[i] = LatentFactors(self.counts)\n",
    "\n",
    "        if index != None:\n",
    "            self.factors.add_member(index)\n",
    "\n",
    "    def remove_factor(self, key: int):\n",
    "        if key in self.factors:\n",
    "            self.factors.pop(key)\n",
    "\n",
    "    def sample_new_factors(self, index):\n",
    "        \"\"\"Sample new features for the given index.\"\"\"\n",
    "        K_new = np.random.poisson(self.alpha / np.log(self.counts.shape[0]))\n",
    "\n",
    "        if K_new > 0:\n",
    "\n",
    "            for _ in range(K_new):\n",
    "\n",
    "                i = 0\n",
    "                while i in self.factors:\n",
    "                    i += 1\n",
    "\n",
    "                self.factors[i] = LatentFactor(self.counts)\n",
    "                self.factors[i].add_member(index)\n",
    "\n",
    "    def remove_empty_factors(self, min_membership=2):\n",
    "        for key in list(self.factors.keys()):  # <-- make a list copy\n",
    "            if self.factors[key].Z.sum() <= min_membership:\n",
    "                self.factors.pop(key)\n",
    "\n",
    "    # This section will contain all the logic used in our Gibbs Sampling\n",
    "    def update_latent_membership(self, index: int):\n",
    "        \"\"\"\n",
    "        Resample membership for all latent factors for a single observation.\n",
    "\n",
    "        Handles the case when no factors currently exist.\n",
    "        \"\"\"\n",
    "        y_n = self.counts[index]         # (D,)\n",
    "        offset_n = self.size_factors[index]  # scalar\n",
    "\n",
    "        # Precompute log-mu with all current features (we’ll subtract them later)\n",
    "        current_log_mu = np.full_like(y_n, offset_n)  # (D,)\n",
    "\n",
    "        for factor in self.factors.values():\n",
    "            if factor.Z[index]:\n",
    "                current_log_mu += factor.A  # sum all current active A_k\n",
    "\n",
    "        for k, factor in self.factors.items():\n",
    "            # Temporarily remove k-th factor's contribution\n",
    "            was_active = factor.Z[index]\n",
    "            if was_active:\n",
    "                log_mu_0 = current_log_mu - factor.A\n",
    "            else:\n",
    "                log_mu_0 = current_log_mu\n",
    "\n",
    "            # Include factor\n",
    "            log_mu_1 = log_mu_0 + factor.A\n",
    "\n",
    "            # Compute log-likelihoods\n",
    "            ll_0 = self.negative_binomial_log_likelihood(y_n, log_mu_0, self.dispersion)\n",
    "            ll_1 = self.negative_binomial_log_likelihood(y_n, log_mu_1, self.dispersion)\n",
    "\n",
    "            # Prior log-odds: based on number of other assignments\n",
    "            m_k = factor.Z.sum() - was_active  # count excluding this index\n",
    "            N = self.counts.shape[0]\n",
    "            log_prior_ratio = np.log((m_k + 1e-10) / (N - m_k + 1e-10))\n",
    "\n",
    "            # Posterior logit and probability\n",
    "            logit_p = log_prior_ratio + (ll_1 - ll_0)\n",
    "            p = 1 / (1 + np.exp(-logit_p))\n",
    "\n",
    "            # Sample new assignment\n",
    "            new_z = np.random.rand() < p\n",
    "\n",
    "            # Update Z and current_log_mu accordingly\n",
    "            if new_z and not was_active:\n",
    "                factor.Z[index] = 1\n",
    "                current_log_mu += factor.A\n",
    "            elif not new_z and was_active:\n",
    "                factor.Z[index] = 0\n",
    "                current_log_mu -= factor.A\n",
    "\n",
    "    def update_latent_loadings(self):\n",
    "        \"\"\"\n",
    "        Update the log-loadings A_k for each latent factor using Polya-Gamma augmentation.\n",
    "        \"\"\"\n",
    "        for k, factor in self.factors.items():\n",
    "            Z_k = factor.Z  # shape (N,)\n",
    "            active_indices = np.where(Z_k == 1)[0]\n",
    "            if len(active_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # Construct X (design matrix): binary indicator (N_active x 1)\n",
    "            X = np.ones((len(active_indices), 1))  # we only have one latent feature per update\n",
    "\n",
    "            # Construct y: (N_active x D)\n",
    "            Y = self.counts[active_indices]  # shape (N_active, D)\n",
    "\n",
    "            # Construct offset: for each sample, subtract other factors' contribution\n",
    "            offset = np.zeros_like(Y)  # shape (N_active, D)\n",
    "            for j, other in self.factors.items():\n",
    "                if j == k:\n",
    "                    continue\n",
    "                offset += np.outer(other.Z[active_indices], other.A)\n",
    "\n",
    "            offset += self.size_factors[active_indices][:, None]\n",
    "\n",
    "            eta = offset + factor.A  # shape (N_active, D)\n",
    "            omega = self.polya_gamma_sampler.sample(Y + self.dispersion, eta)\n",
    "\n",
    "            # Posterior covariance and mean for each gene d\n",
    "            A_k_new = np.zeros_like(factor.A)\n",
    "            for d in range(self.counts.shape[1]):\n",
    "                XWX = (X.T * omega[:, d]) @ X\n",
    "                precision = XWX + self.prior_precision[d]\n",
    "                posterior_cov = 1.0 / precision\n",
    "\n",
    "                # Posterior mean\n",
    "                update_grad = np.sum(X.T * (Y[:, d] - self.dispersion[d]) / 2)\n",
    "                prior_contrib = self.prior_precision[d] * 0.0  # mean is 0\n",
    "                posterior_mean = posterior_cov * (update_grad + prior_contrib)\n",
    "\n",
    "                A_k_new[d] = np.random.normal(posterior_mean, np.sqrt(posterior_cov))\n",
    "\n",
    "            factor.A = A_k_new\n",
    "\n",
    "\n",
    "    def update_dispersion(self, proposal_sigma=1.0):\n",
    "        \"\"\"Update each gene's dispersion parameter via simple MH step.\"\"\"\n",
    "        for d in range(self.counts.shape[1]):\n",
    "            r_old = self.dispersion[d]\n",
    "            r_prop = np.abs(r_old + np.random.normal(0, proposal_sigma))  # reflect at 0\n",
    "\n",
    "            loglike_old = 0.0\n",
    "            loglike_prop = 0.0\n",
    "\n",
    "            for n in range(self.counts.shape[0]):\n",
    "                # Compute current log-mu for this observation\n",
    "                log_mu = self.size_factors[n]\n",
    "                for factor in self.factors.values():\n",
    "                    if factor.Z[n]:\n",
    "                        log_mu += factor.A[d]\n",
    "\n",
    "                y_nd = self.counts[n, d]\n",
    "\n",
    "                loglike_old += self.negative_binomial_log_likelihood(\n",
    "                    np.array([y_nd]), np.array([log_mu]), np.array([r_old])\n",
    "                )\n",
    "                loglike_prop += self.negative_binomial_log_likelihood(\n",
    "                    np.array([y_nd]), np.array([log_mu]), np.array([r_prop])\n",
    "                )\n",
    "\n",
    "            # Optional prior on dispersion — Gamma(r | a0, b0)\n",
    "            a0, b0 = 2.0, 0.1\n",
    "            prior_old = (a0 - 1) * np.log(r_old) - b0 * r_old\n",
    "            prior_prop = (a0 - 1) * np.log(r_prop) - b0 * r_prop\n",
    "\n",
    "            log_accept_ratio = (loglike_prop + prior_prop) - (loglike_old + prior_old)\n",
    "            if np.log(np.random.rand()) < log_accept_ratio:\n",
    "                self.dispersion[d] = r_prop  # accept\n",
    "\n",
    "    # Actual Gibbs Sampling Logic.\n",
    "    def gibbs_sampling(self, n_epochs: int = 1000, verbose = True):\n",
    "        # For the current epoch.\n",
    "        for current_epoch in range(n_epochs):\n",
    "\n",
    "            # 1. Update membership and sample new factors for each observation\n",
    "            for index in trange(self.counts.shape[0]):\n",
    "                self.update_latent_membership(index)\n",
    "                self.sample_new_factors(index)\n",
    "\n",
    "            # 2. Update loadings for each factor\n",
    "            self.update_latent_loadings()\n",
    "\n",
    "            # 3. Update dispersions\n",
    "            self.update_dispersion()\n",
    "\n",
    "            # 4. Remove any now-empty factors\n",
    "            \n",
    "            self.remove_empty_factors(min_membership=max(2 * np.log(current_epoch), 1))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {current_epoch}, # of Features = {len(self.factors)}, log-likelihood = {self.log_likelihood()}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def negative_binomial_log_likelihood(counts: np.ndarray, log_mu: np.ndarray, dispersion: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute NB log-likelihood for a single observation across D dimensions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        counts : (D,) array\n",
    "            Observed counts for one sample.\n",
    "        log_mu : (D,) array\n",
    "            Log of mean counts.\n",
    "        dispersion : (D,) array\n",
    "            Dispersion parameters for NB.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Sum of log-likelihoods over D dimensions.\n",
    "        \"\"\"\n",
    "        mu = np.exp(log_mu)\n",
    "        r = dispersion\n",
    "        y = counts\n",
    "\n",
    "        return np.sum(\n",
    "            gammaln(y + r) - gammaln(r) - gammaln(y + 1) +\n",
    "            r * np.log(r / (r + mu)) +\n",
    "            y * np.log(mu / (r + mu))\n",
    "        )\n",
    "\n",
    "    def log_likelihood(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood of the entire dataset under the model.\n",
    "\n",
    "        Sums over all samples n and genes d:\n",
    "        log p(y_nd | Z, A, dispersion, size_factors)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Total log-likelihood scalar.\n",
    "        \"\"\"\n",
    "        total_ll = 0.0\n",
    "        N, D = self.counts.shape\n",
    "\n",
    "        for n in range(N):\n",
    "            # Compute log_mu for sample n\n",
    "            log_mu = np.full(D, self.size_factors[n])\n",
    "            for factor in self.factors.values():\n",
    "                if factor.Z[n]:\n",
    "                    log_mu += factor.A\n",
    "\n",
    "            # Counts for sample n\n",
    "            y_n = self.counts[n]\n",
    "\n",
    "            # Sum Negative Binomial log-likelihood for this sample\n",
    "            total_ll += self.negative_binomial_log_likelihood(y_n, log_mu, self.dispersion)\n",
    "\n",
    "        return total_ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f28f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactor:\n",
    "\n",
    "    def __init__(self, counts: np.array):\n",
    "        # Store a reference to the counts just for fun.\n",
    "        self.counts = counts\n",
    "\n",
    "        # Latent Membership\n",
    "        # How should this be initialized for logits construction.\n",
    "        # Simplest case, we'll make this binary at first.\n",
    "        # ChatGPT says Albert & Chib (1993) is good?\n",
    "        self.Z = np.zeros(counts.shape[0])\n",
    "\n",
    "        # Latent Loadings\n",
    "        # My first instinct is gaussian. Starting off with this.\n",
    "        self.A = np.zeros(counts.shape[1])\n",
    "    \n",
    "    def add_member(self, index):\n",
    "        \"\"\"Just add the index\"\"\"\n",
    "        # What else you gotta say?\n",
    "        self.Z[index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cf7972ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:00<00:00, 1235.73it/s]\n",
      "/tmp/ipykernel_690991/3611216892.py:150: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  A_k_new[d] = np.random.normal(posterior_mean, np.sqrt(posterior_cov))\n",
      "/tmp/ipykernel_690991/3611216892.py:207: RuntimeWarning: divide by zero encountered in log\n",
      "  self.remove_empty_factors(min_membership=max(2 * np.log(current_epoch), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, # of Features = 8, log-likelihood = -736567.5552646157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/808 [00:00<?, ?it/s]/tmp/ipykernel_690991/3611216892.py:97: RuntimeWarning: overflow encountered in exp\n",
      "  p = 1 / (1 + np.exp(-logit_p))\n",
      "100%|██████████| 808/808 [00:00<00:00, 1096.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, # of Features = 13, log-likelihood = -342346.3627194388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 603.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, # of Features = 20, log-likelihood = -322879.3937728793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 438.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, # of Features = 13, log-likelihood = -321660.23025927827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 628.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, # of Features = 15, log-likelihood = -304573.23373357527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 610.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, # of Features = 11, log-likelihood = -300422.0988268419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 681.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, # of Features = 13, log-likelihood = -289709.88834416895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 603.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, # of Features = 14, log-likelihood = -282554.49387919565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 638.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, # of Features = 7, log-likelihood = -290704.5307119689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:00<00:00, 836.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, # of Features = 7, log-likelihood = -272658.50756252673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:00<00:00, 1078.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, # of Features = 7, log-likelihood = -268796.003308564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 780.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, # of Features = 7, log-likelihood = -266110.33207483473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:00<00:00, 815.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, # of Features = 8, log-likelihood = -265419.9644792972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:01<00:00, 726.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, # of Features = 6, log-likelihood = -273276.0028518394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:00<00:00, 1047.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m ibp = IndianBuffetProcess(np.array(X)[::\u001b[32m5\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mibp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgibbs_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mIndianBuffetProcess.gibbs_sampling\u001b[39m\u001b[34m(self, n_epochs, verbose)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.update_latent_loadings()\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# 3. Update dispersions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_dispersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# 4. Remove any now-empty factors\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.remove_empty_factors(min_membership=\u001b[38;5;28mmax\u001b[39m(\u001b[32m2\u001b[39m * np.log(current_epoch), \u001b[32m1\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36mIndianBuffetProcess.update_dispersion\u001b[39m\u001b[34m(self, proposal_sigma)\u001b[39m\n\u001b[32m    169\u001b[39m             log_mu += factor.A[d]\n\u001b[32m    171\u001b[39m     y_nd = \u001b[38;5;28mself\u001b[39m.counts[n, d]\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     loglike_old += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnegative_binomial_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_nd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlog_mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr_old\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     loglike_prop += \u001b[38;5;28mself\u001b[39m.negative_binomial_log_likelihood(\n\u001b[32m    177\u001b[39m         np.array([y_nd]), np.array([log_mu]), np.array([r_prop])\n\u001b[32m    178\u001b[39m     )\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# Optional prior on dispersion — Gamma(r | a0, b0)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 238\u001b[39m, in \u001b[36mIndianBuffetProcess.negative_binomial_log_likelihood\u001b[39m\u001b[34m(counts, log_mu, dispersion)\u001b[39m\n\u001b[32m    232\u001b[39m r = dispersion\n\u001b[32m    233\u001b[39m y = counts\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.sum(\n\u001b[32m    236\u001b[39m     gammaln(y + r) - gammaln(r) - gammaln(y + \u001b[32m1\u001b[39m) +\n\u001b[32m    237\u001b[39m     r * np.log(r / (r + mu)) +\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     y * \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ibp = IndianBuffetProcess(np.array(X)[::5])\n",
    "ibp.gibbs_sampling(n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fdf5d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 747.0\n",
      "2 30.0\n",
      "8 7.0\n",
      "17 7.0\n",
      "12 6.0\n",
      "1 11.0\n",
      "3 7.0\n",
      "4 1.0\n",
      "5 1.0\n",
      "6 2.0\n",
      "7 1.0\n",
      "9 2.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "13 1.0\n",
      "14 2.0\n",
      "15 1.0\n",
      "16 1.0\n",
      "18 1.0\n",
      "19 1.0\n",
      "20 1.0\n"
     ]
    }
   ],
   "source": [
    "for k in ibp.factors:\n",
    "    print(k, ibp.factors[k].Z.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "356e3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top5_abs_indices(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the indices of the 5 elements with largest absolute values in `arr`.\n",
    "    If `arr` has fewer than 5 elements, returns all indices sorted by abs value descending.\n",
    "    \"\"\"\n",
    "    n = min(5, arr.size)\n",
    "    # argsort returns indices sorted ascending, so take last n and reverse\n",
    "    return np.argsort(arr)[-n:][::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3844b",
   "metadata": {},
   "source": [
    "The first group is an intercept basically, this ones a gimme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de6dd32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VCAN', 'MARCO', 'HSP90AA1', 'CD83', 'F13A1'], dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var_names[top5_abs_indices(ibp.factors[0].A)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65b270",
   "metadata": {},
   "source": [
    "This one shows that its for real. I've seen PADI2, HSPs and FTL before, so I'm fairly certain this has some meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "904138ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HSP90AA1', 'FTL', 'NDRG1', 'HSPH1', 'PADI2'], dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var_names[top5_abs_indices(ibp.factors[2].A)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eefd75",
   "metadata": {},
   "source": [
    "This is a smaller one, and just as an example, AQP9 is primarily expressed in vasculature and macrophages. Given the associated VCAN signature, you might call this something like \"vasculature associated macrophages\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6ebae0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VCAN', 'AC037198.1', 'TFRC', 'ALPK3', 'AQP9'], dtype='object')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var_names[top5_abs_indices(ibp.factors[1].A)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0195427",
   "metadata": {},
   "source": [
    "So our sampler now works! and gives us some kind of unique and biologically interesting factors! It will take a generation to learn anything, but thats my fault."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
