{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8aec32",
   "metadata": {},
   "source": [
    "# Broad Structure\n",
    "\n",
    "Our sampler is going to have three components to it:\n",
    "\n",
    "1. Learning the sparse latent matrix\n",
    "    - We will encode this as draws from a bernoulli probability (logits).\n",
    "    - We should be able to use a polya gamma construction for this!\n",
    "\n",
    "2. Learning the loading matrix\n",
    "    - We will use our polya gamma construction here as well.\n",
    "\n",
    "3. Dispersion parameter\n",
    "    - Probably, we will use simple MH type updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8957dac",
   "metadata": {},
   "source": [
    "# Indian Buffet Process Class\n",
    "\n",
    "I think it should be a container for our latent factors, which will have all the coefficients and stuff. However, the math stuff will definitely be done on this thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndianBuffetProcess:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, counts: np.array, alpha: float = 1.0):\n",
    "        # Count Matrix, np.array\n",
    "        self.counts = counts\n",
    "\n",
    "        # Innovation Parameter\n",
    "        self.alpha = alpha / np.log(self.counts.shape[0])\n",
    "\n",
    "        # Latent Factor Matrix.\n",
    "        self.factors = {}\n",
    "\n",
    "        # Our prior dispersion parameter, right now it will be 5.0, but this can change.\n",
    "        self.dispersion = np.ones(self.counts.shape[1]) * 5.0\n",
    "    \n",
    "    def add_factor(self):\n",
    "        pass\n",
    "\n",
    "    def remove_factor(self):\n",
    "        pass\n",
    "\n",
    "    # This section will contain all the logic used in our Gibbs Sampling\n",
    "    def update_latent_membership(self):\n",
    "        # this will include logic for either sampling new memberships \n",
    "        pass\n",
    "\n",
    "    def update_latent_loadings(self):\n",
    "        pass\n",
    "\n",
    "    def update_dispersion(self):\n",
    "        pass\n",
    "\n",
    "    # Actual Gibbs Sampling Logic.\n",
    "    def gibbs_sampling(self, n_epochs: int = 1000, verbose = True):\n",
    "        for current_epoch in trange(n_epochs):\n",
    "            self.update_latent_membership()\n",
    "            self.update_latent_loadings()\n",
    "            self.update_dispersion()\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Epoch {current_epoch}, log-likelhood = \": self.log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactor:\n",
    "\n",
    "    def __init__(self, counts: np.array):\n",
    "        self.counts = counts\n",
    "\n",
    "        # Latent Membership\n",
    "        # How should this be initialized for logits construction.\n",
    "        # My first instinct is gaussian.\n",
    "        self.Z = np.zeros(counts.shape[0])\n",
    "\n",
    "        # Latent Loadings\n",
    "        # My first instinct is gaussian. Starting off with this.\n",
    "        self.A = np.zeros(counts.shape[1])\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
