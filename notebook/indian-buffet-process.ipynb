{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8aec32",
   "metadata": {},
   "source": [
    "# Broad Structure\n",
    "\n",
    "Our sampler is going to have three components to it:\n",
    "\n",
    "1. Learning the sparse latent matrix\n",
    "    - We will encode this as draws from a bernoulli probability (logits).\n",
    "    - We should be able to use a polya gamma construction for this!\n",
    "\n",
    "2. Learning the loading matrix\n",
    "    - We will use our polya gamma construction here as well.\n",
    "\n",
    "3. Dispersion parameter\n",
    "    - Probably, we will use simple MH type updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8957dac",
   "metadata": {},
   "source": [
    "# Indian Buffet Process Class\n",
    "\n",
    "I think it should be a container for our latent factors, which will have all the coefficients and stuff. However, the math stuff will definitely be done on this thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ad604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhaberbe/Projects/Personal/nb-ibp/.venv/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:172: ImplicitModificationWarning: Trying to modify attribute `._uns` of view, initializing view as actual.\n",
      "  adata.uns[\"hvg\"] = {\"flavor\": flavor}\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "adata = sc.read_h5ad(\"/home/jhaberbe/Data/choroid-plexus/new_annotations.h5ad\")\n",
    "adata = adata[adata.X.sum(axis=1) > 300]\n",
    "adata = adata[adata.obs[\"Cell.Subtype\"].eq(\"Macrophage\")][::5]\n",
    "sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=1000, subset=True, inplace=True)\n",
    "\n",
    "X = adata.X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2123479",
   "metadata": {},
   "source": [
    "Actual sampling time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37409b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "319f7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyaGammaSampler:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sample_scalar(self, b, c, truncation=100):\n",
    "        total = 0.0\n",
    "        for k in range(1, truncation + 1):\n",
    "            term_1 = np.random.gamma(b, 1.0)\n",
    "            term_2 = (k - 0.5)**2\n",
    "            term_3 = (c / (2 * np.pi))**2\n",
    "            total += term_1 / (term_2 + term_3)\n",
    "        \n",
    "        return 0.5 * total / (np.pi**2)\n",
    "\n",
    "    # def sample(self, b_array, c_array, truncation=100):\n",
    "    #     b_array = np.asarray(b_array)\n",
    "    #     c_array = np.asarray(c_array)\n",
    "    #     N = b_array.shape[0]\n",
    "    #     omega = np.zeros(N)\n",
    "        \n",
    "    #     for n in range(N):\n",
    "    #         omega[n] = self.sample_scalar(b_array[n], c_array[n], truncation=truncation)\n",
    "        \n",
    "    #     return omega\n",
    "\n",
    "    def sample(self, b_array, c_array, truncation=100):\n",
    "        b_array = np.asarray(b_array)\n",
    "        c_array = np.asarray(c_array)\n",
    "        omega = np.zeros_like(b_array, dtype=float)\n",
    "\n",
    "        # Flatten arrays to loop over all elements (n, d)\n",
    "        b_flat = b_array.flatten()\n",
    "        c_flat = c_array.flatten()\n",
    "\n",
    "        omega_flat = np.zeros_like(b_flat, dtype=float)\n",
    "\n",
    "        for i in range(len(b_flat)):\n",
    "            omega_flat[i] = self.sample_scalar(b_flat[i], c_flat[i], truncation=truncation)\n",
    "\n",
    "        omega = omega_flat.reshape(b_array.shape)\n",
    "        return omega\n",
    "\n",
    "\n",
    "pg = PolyaGammaSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79dfbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndianBuffetProcess:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, counts: np.array, alpha: float = 1.0):\n",
    "        # Count Matrix, np.array\n",
    "        self.counts = counts\n",
    "        self.size_factors = np.log(counts.sum(axis=1) / counts.sum(axis=1).mean())\n",
    "\n",
    "        # Innovation Parameter\n",
    "        self.alpha = alpha / np.log(self.counts.shape[0])\n",
    "\n",
    "        # Latent Factor Matrix.\n",
    "        self.factors = {}\n",
    "\n",
    "        # Our prior dispersion parameter, right now it will be 5.0, but this can change.\n",
    "        self.dispersion = np.ones(self.counts.shape[1]) * 5.0\n",
    "\n",
    "        # Loading updates stuff for polya gamma sampler.\n",
    "        self.polya_gamma_sampler = PolyaGammaSampler()\n",
    "        self.prior_precision = 0.01 * np.ones(self.counts.shape[1])  # Precision vector for A_k\n",
    "\n",
    "\n",
    "    # This section will deal with factors broadly.\n",
    "    def add_factor(self, index: Union[None, int]):\n",
    "        i = 0\n",
    "        while i in self.factors:\n",
    "            i += 1\n",
    "\n",
    "        self.factors[i] = LatentFactors(self.counts)\n",
    "\n",
    "        if index != None:\n",
    "            self.factors.add_member(index)\n",
    "\n",
    "    def remove_factor(self, key: int):\n",
    "        if key in self.factors:\n",
    "            self.factors.pop(key)\n",
    "\n",
    "    def sample_new_factors(self, index):\n",
    "        \"\"\"Sample new features for the given index.\"\"\"\n",
    "        K_new = np.random.poisson(self.alpha / np.log(self.counts.shape[0]))\n",
    "\n",
    "        if K_new > 0:\n",
    "\n",
    "            for _ in range(K_new):\n",
    "\n",
    "                i = 0\n",
    "                while i in self.factors:\n",
    "                    i += 1\n",
    "\n",
    "                self.factors[i] = LatentFactor(self.counts)\n",
    "                self.factors[i].add_member(index)\n",
    "\n",
    "    def remove_empty_factors(self):\n",
    "        for key in self.factors:\n",
    "            if self.factors[key].Z.sum() == 0:\n",
    "                self.factors.pop(key)\n",
    "\n",
    "    # This section will contain all the logic used in our Gibbs Sampling\n",
    "    def update_latent_membership(self, index: int):\n",
    "        \"\"\"\n",
    "        Resample membership for all latent factors for a single observation.\n",
    "\n",
    "        Handles the case when no factors currently exist.\n",
    "        \"\"\"\n",
    "        y_n = self.counts[index]         # (D,)\n",
    "        offset_n = self.size_factors[index]  # scalar\n",
    "\n",
    "        # Precompute log-mu with all current features (weâ€™ll subtract them later)\n",
    "        current_log_mu = np.full_like(y_n, offset_n)  # (D,)\n",
    "\n",
    "        for factor in self.factors.values():\n",
    "            if factor.Z[index]:\n",
    "                current_log_mu += factor.A  # sum all current active A_k\n",
    "\n",
    "        for k, factor in self.factors.items():\n",
    "            # Temporarily remove k-th factor's contribution\n",
    "            was_active = factor.Z[index]\n",
    "            if was_active:\n",
    "                log_mu_0 = current_log_mu - factor.A\n",
    "            else:\n",
    "                log_mu_0 = current_log_mu\n",
    "\n",
    "            # Include factor\n",
    "            log_mu_1 = log_mu_0 + factor.A\n",
    "\n",
    "            # Compute log-likelihoods\n",
    "            ll_0 = self.negative_binomial_log_likelihood(y_n, log_mu_0, self.dispersion)\n",
    "            ll_1 = self.negative_binomial_log_likelihood(y_n, log_mu_1, self.dispersion)\n",
    "\n",
    "            # Prior log-odds: based on number of other assignments\n",
    "            m_k = factor.Z.sum() - was_active  # count excluding this index\n",
    "            N = self.counts.shape[0]\n",
    "            log_prior_ratio = np.log((m_k + 1e-10) / (N - m_k + 1e-10))\n",
    "\n",
    "            # Posterior logit and probability\n",
    "            logit_p = log_prior_ratio + (ll_1 - ll_0)\n",
    "            p = 1 / (1 + np.exp(-logit_p))\n",
    "\n",
    "            # Sample new assignment\n",
    "            new_z = np.random.rand() < p\n",
    "\n",
    "            # Update Z and current_log_mu accordingly\n",
    "            if new_z and not was_active:\n",
    "                factor.Z[index] = 1\n",
    "                current_log_mu += factor.A\n",
    "            elif not new_z and was_active:\n",
    "                factor.Z[index] = 0\n",
    "                current_log_mu -= factor.A\n",
    "\n",
    "    def update_latent_loadings(self):\n",
    "        \"\"\"\n",
    "        Update the log-loadings A_k for each latent factor using Polya-Gamma augmentation.\n",
    "        \"\"\"\n",
    "        for k, factor in self.factors.items():\n",
    "            Z_k = factor.Z  # shape (N,)\n",
    "            active_indices = np.where(Z_k == 1)[0]\n",
    "            if len(active_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # Construct X (design matrix): binary indicator (N_active x 1)\n",
    "            X = np.ones((len(active_indices), 1))  # we only have one latent feature per update\n",
    "\n",
    "            # Construct y: (N_active x D)\n",
    "            Y = self.counts[active_indices]  # shape (N_active, D)\n",
    "\n",
    "            # Construct offset: for each sample, subtract other factors' contribution\n",
    "            offset = np.zeros_like(Y)  # shape (N_active, D)\n",
    "            for j, other in self.factors.items():\n",
    "                if j == k:\n",
    "                    continue\n",
    "                offset += np.outer(other.Z[active_indices], other.A)\n",
    "\n",
    "            offset += self.size_factors[active_indices][:, None]\n",
    "\n",
    "            eta = offset + factor.A  # shape (N_active, D)\n",
    "            omega = self.polya_gamma_sampler.sample(Y + self.dispersion, eta)\n",
    "\n",
    "            # Posterior covariance and mean for each gene d\n",
    "            A_k_new = np.zeros_like(factor.A)\n",
    "            for d in range(self.counts.shape[1]):\n",
    "                XWX = (X.T * omega[:, d]) @ X\n",
    "                precision = XWX + self.prior_precision[d]\n",
    "                posterior_cov = 1.0 / precision\n",
    "\n",
    "                # Posterior mean\n",
    "                update_grad = np.sum(X.T * (Y[:, d] - self.dispersion[d]) / 2)\n",
    "                prior_contrib = self.prior_precision[d] * 0.0  # mean is 0\n",
    "                posterior_mean = posterior_cov * (update_grad + prior_contrib)\n",
    "\n",
    "                A_k_new[d] = np.random.normal(posterior_mean, np.sqrt(posterior_cov))\n",
    "\n",
    "            factor.A = A_k_new\n",
    "\n",
    "\n",
    "    def update_dispersion(self, proposal_sigma=1.0):\n",
    "        \"\"\"Update each gene's dispersion parameter via simple MH step.\"\"\"\n",
    "        for d in range(self.counts.shape[1]):\n",
    "            r_old = self.dispersion[d]\n",
    "            r_prop = np.abs(r_old + np.random.normal(0, proposal_sigma))  # reflect at 0\n",
    "\n",
    "            loglike_old = 0.0\n",
    "            loglike_prop = 0.0\n",
    "\n",
    "            for n in range(self.counts.shape[0]):\n",
    "                # Compute current log-mu for this observation\n",
    "                log_mu = self.size_factors[n]\n",
    "                for factor in self.factors.values():\n",
    "                    if factor.Z[n]:\n",
    "                        log_mu += factor.A[d]\n",
    "\n",
    "                y_nd = self.counts[n, d]\n",
    "\n",
    "                loglike_old += self.negative_binomial_log_likelihood(\n",
    "                    np.array([y_nd]), np.array([log_mu]), np.array([r_old])\n",
    "                )\n",
    "                loglike_prop += self.negative_binomial_log_likelihood(\n",
    "                    np.array([y_nd]), np.array([log_mu]), np.array([r_prop])\n",
    "                )\n",
    "\n",
    "            # Optional prior on dispersion â€” Gamma(r | a0, b0)\n",
    "            a0, b0 = 2.0, 0.1\n",
    "            prior_old = (a0 - 1) * np.log(r_old) - b0 * r_old\n",
    "            prior_prop = (a0 - 1) * np.log(r_prop) - b0 * r_prop\n",
    "\n",
    "            log_accept_ratio = (loglike_prop + prior_prop) - (loglike_old + prior_old)\n",
    "            if np.log(np.random.rand()) < log_accept_ratio:\n",
    "                self.dispersion[d] = r_prop  # accept\n",
    "\n",
    "    # Actual Gibbs Sampling Logic.\n",
    "    def gibbs_sampling(self, n_epochs: int = 1000, verbose = True):\n",
    "        # For the current epoch.\n",
    "        for current_epoch in range(n_epochs):\n",
    "\n",
    "            # 1. Update membership and sample new factors for each observation\n",
    "            for index in trange(self.counts.shape[0]):\n",
    "                self.update_latent_membership(index)\n",
    "                self.sample_new_factors(index)\n",
    "\n",
    "            # 2. Update loadings for each factor\n",
    "            self.update_latent_loadings()\n",
    "\n",
    "            # 3. Update dispersions\n",
    "            self.update_dispersion()\n",
    "\n",
    "            # 4. Remove any now-empty factors\n",
    "            self.remove_empty_factors()\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {current_epoch}, log-likelihood = {self.log_likelihood()}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def negative_binomial_log_likelihood(counts: np.ndarray, log_mu: np.ndarray, dispersion: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute NB log-likelihood for a single observation across D dimensions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        counts : (D,) array\n",
    "            Observed counts for one sample.\n",
    "        log_mu : (D,) array\n",
    "            Log of mean counts.\n",
    "        dispersion : (D,) array\n",
    "            Dispersion parameters for NB.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Sum of log-likelihoods over D dimensions.\n",
    "        \"\"\"\n",
    "        mu = np.exp(log_mu)\n",
    "        r = dispersion\n",
    "        y = counts\n",
    "\n",
    "        return np.sum(\n",
    "            gammaln(y + r) - gammaln(r) - gammaln(y + 1) +\n",
    "            r * np.log(r / (r + mu)) +\n",
    "            y * np.log(mu / (r + mu))\n",
    "        )\n",
    "\n",
    "    def log_likelihood(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood of the entire dataset under the model.\n",
    "\n",
    "        Sums over all samples n and genes d:\n",
    "        log p(y_nd | Z, A, dispersion, size_factors)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Total log-likelihood scalar.\n",
    "        \"\"\"\n",
    "        total_ll = 0.0\n",
    "        N, D = self.counts.shape\n",
    "\n",
    "        for n in range(N):\n",
    "            # Compute log_mu for sample n\n",
    "            log_mu = np.full(D, self.size_factors[n])\n",
    "            for factor in self.factors.values():\n",
    "                if factor.Z[n]:\n",
    "                    log_mu += factor.A\n",
    "\n",
    "            # Counts for sample n\n",
    "            y_n = self.counts[n]\n",
    "\n",
    "            # Sum Negative Binomial log-likelihood for this sample\n",
    "            total_ll += self.negative_binomial_log_likelihood(y_n, log_mu, self.dispersion)\n",
    "\n",
    "        return total_ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f28f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactor:\n",
    "\n",
    "    def __init__(self, counts: np.array):\n",
    "        # Store a reference to the counts just for fun.\n",
    "        self.counts = counts\n",
    "\n",
    "        # Latent Membership\n",
    "        # How should this be initialized for logits construction.\n",
    "        # Simplest case, we'll make this binary at first.\n",
    "        # ChatGPT says Albert & Chib (1993) is good?\n",
    "        self.Z = np.zeros(counts.shape[0])\n",
    "\n",
    "        # Latent Loadings\n",
    "        # My first instinct is gaussian. Starting off with this.\n",
    "        self.A = np.zeros(counts.shape[1])\n",
    "    \n",
    "    def add_member(self, index):\n",
    "        \"\"\"Just add the index\"\"\"\n",
    "        # What else you gotta say?\n",
    "        self.Z[index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7972ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibp = IndianBuffetProcess(np.array(X))\n",
    "ibp.gibbs_sampling(n_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
